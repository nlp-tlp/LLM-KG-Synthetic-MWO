# Evaluation

The quality of the synthetic MWO data generated is evaluated using the following methods:

1. [Paths and Synthetic MWO Analysis](#paths-and-synthetic-mwo-analysis)
2. [Turing Test](#turing-test)
3. [Ranking Test](#ranking-test)

## Paths and Synthetic MWO Analysis

### Number of Valid Paths 

We extracted 2,237 valid equipment-failure paths from the MaintIE KG, with the most common being State-Patient (1,208 paths). After SME validation, the number increased to 3,794, with significant gains in State-Agent-Activity and Process-Agent-Patient paths. The following table shows the number of valid paths before and after SME validation:

| Path Type               | Number of Valid Paths | Direct | Additional | Total before SME Validation | Total after SME Validation |
|-------------------------|-----------------------|--------|------------|-----------------------------|----------------------------|
| Object-Property         | 26                    | 23     | 49         | 49                          | 49                         |
| Process-Agent           | 77                    | 314    | 391        | 391                         | 391                        |
| Process-Patient         | 38                    | 231    | 269        | 269                         | 269                        |
| State-Patient           | 293                   | 915    | 1208       | 1208                        | 1208                       |
| Object-Property-State   | 1                     | 2      | 3          | 3                           | 3                          |
| State-Agent-Activity    | 25                    | 111    | 136        | 607                         | 607                        |
| State-Agent-Patient     | 7                     | 24     | 31         | 37                          | 37                         |
| Process-Agent-Patient   | 31                    | 119    | 150        | 1230                        | 1230                       |
| **Total**               | **498**               | **1739**| **2237**   | **3794**                     | **3794**                    |

Note: The code and results for path extraction can be found in the [`PathExtraction/path_matching.ipynb`](https://github.com/nlp-tlp/Hons24_AllisonLau/blob/main/PathExtraction/path_matching.ipynb) notebook.

### Path Distribution

An analysis of failure events and objects in the extracted paths revealed significant imbalance, with certain events and components overrepresented. For example, *leaking* accounts for 43% of paths, and *hose* appears in 34%. In contrast, events like *noisy* and *corroded* are underrepresented. Addressing this imbalance through sampling strategies is needed to create more balanced datasets.

| Undesirable Event    | Occurrence      | Physical Object   | Occurrence      |
|----------------------|-----------------|-------------------|-----------------|
| Leaking              | 1700 / 3975     | Hose              | 1364 / 3975     |
| Unserviceable        | 241 / 3975      | Pump              | 297 / 3975      |
| Need replacing       | 238 / 3975      | Air conditioner   | 103 / 3975      |

Note: The code and results for path distribution analysis can be found in the [`PathExtraction/path_matching.ipynb`](https://github.com/nlp-tlp/Hons24_AllisonLau/blob/main/PathExtraction/path_matching.ipynb) notebook.

### Sentence Length Distribution

In this analysis, we compared the sentence token distribution between human MWOs and synthetic MWOs generated by the LLM. The human MWO sentences comprised approximately 17,000 entries from the MaintIE gold and silver datasets and the MaintNorm dataset, while the synthetic MWOs totaled 10,000 sentences from various generations. The following graph shows the minimum, maximum, and average number of tokens per sentence for both datasets. 

![Sentence Length Distribution](Images/num_tokens.png)

Note: The code and results for sentence length distribution analysis can be found in the [`DataAnalysis/synthetic_analysis.ipynb`](https://github.com/nlp-tlp/Hons24_AllisonLau/blob/main/DataAnalysis/synthetic_analysis.ipynb) notebook.

## Turing Test

To evaluate the authenticity of our synthetic maintenance work order (MWO) sentences, we conducted a Turing test involving subject matter experts (SMEs) in the maintenance and asset management industry. The goal was to assess whether experts could distinguish between real human-written MWOs and synthetic, LLM-generated MWOs that had been “humanized.”

### Experiment Setup

- **100 sentences total** (50 human-written, 50 synthetic), drawn from the MaintNorm dataset and LLM outputs.
- **7 industry experts** (5–35 years of experience) were asked to label each sentence as either "human" or "synthetic."
- Annotators were aware of the 50:50 split but not which individual sentences were which.
- Two iterations of the test were conducted to check for consistency.

### Key Metrics

| Metric                                | Iteration 1 | Iteration 2 |
|---------------------------------------|-------------|-------------|
| **Average Accuracy**                  | 0.50        | 0.52        |
| **Proportion of Synthetic as Human**  | 0.50        | 0.48        |
| **Krippendorff’s α (Agreement)**      | -0.0005     | -0.0024     |

Low agreement and near-random accuracy suggest that most experts were unable to reliably distinguish between real and synthetic MWOs.

### Statistical Analysis

A likelihood ratio test (LRT) for each expert evaluated the **independence hypothesis**—whether their classifications were independent of the true sentence origin.

- **Only 1 out of 7 experts (F)** consistently rejected the independence hypothesis *with high power* (≥ 80%) and achieved >50% accuracy.
- For the **population of experts**, the average accuracy was:
  - **0.497** (iteration 1)
  - **0.520** (iteration 2)

These results indicate that experts, on average, performed no better than chance. LRTs on population-level accuracy yielded **no significant difference from 50%**, confirming that the task was effectively indistinguishable from random guessing.

Note: Code and results for the Turing test can be found in the [`Evaluation/evaluation.ipynb`](https://github.com/nlp-tlp/Hons24_AllisonLau/blob/main/Evaluation/evaluation.ipynb)notebook.

## Ranking Test

Replicated the evaluation from [Bikaun et al. (2022)](https://github.com/nlp-tlp/cfg_technical_short_text). The synthetic data achieved a correctness score of **4.04 out of 5**, indicating effective grounding in retaining engineering relationships. The synthetic sentences were evaluated as **86%** as natural and **95%** as correct compared to their human counterparts.

| Attribute     | Human         | Synthetic   |
|---------------|---------------|-------------|
| Naturalness   | 4.44 / 5      | 3.80 / 5    |
| Correctness   | 4.24 / 5      | 4.04 / 5    |

Note: Code and results for the Ranking test can be found in the [`Evaluation/evaluation.ipynb`](https://github.com/nlp-tlp/Hons24_AllisonLau/blob/main/Evaluation/evaluation.ipynb) notebook.